{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bodi44/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 \n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_url = \"chrome://dino\"\n",
    "chrome_driver_path = \"/usr/bin/chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get('chrome://dino')\n",
    "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): \n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n",
      "fps: 0.37936920195785184\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 31.07950857329164\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.28871439568899\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 31.239462845311067\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.88154408190784\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 31.988773471224395\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.425004251897896\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 27.0976128177795\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 25.36560349311175\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.491038720881996\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.05821116835074\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.48477338207503\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.558405266099484\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 34.17477246987314\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 24.70129151182855\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 22.238915812133488\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 24.17299091705473\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 17.199074909377202\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 21.442394994069772\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 28.551326036050753\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 27.759934344637703\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.979014781141892\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 31.79454058930101\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.703359765463574\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 28.70274894100418\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 34.63304350698143\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 32.16022205353514\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 28.7053026362616\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 28.117799274648217\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 33.96445085066928\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 35.55187875602872\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 22.274109949868297\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.11938328957113\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 19.78407954566895\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 17.542636307362855\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 18.5187978224108\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 8.37163033368329\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.09109463464133\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 5.486357696629287\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.603720742946605\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.138298272573891\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 10.940787345707996\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.332017111859225\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 11.863094985561109\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.7036218144812\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 7.84473268308193\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 10.514859874703241\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 11.726906462751458\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 21.747808006802828\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 21.03124874643989\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 24.56861022270645\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 19.439313323816762\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 23.684054795758186\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 22.58268140482741\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 23.38809490618117\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 27.521860379661284\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 11.278740013499087\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 24.244111373791206\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.968697917596046\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 23.391747115282726\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 23.443522198187924\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 27.8024406572938\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 27.267256959342618\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 30.417091512984705\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.459140170111745\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 11.057458234362107\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.21455852998092\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 16.101593151368576\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.983232921396697\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.280204948031034\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.168889033550688\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.602393867021314\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.373549441768747\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 22.7451899091126\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 24.963569162644255\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.818071421298164\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.811915794199226\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.638367523470764\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.0611619564853\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 17.065139025640608\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 30.82188680354492\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.759944821373272\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 15.043754281635684\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.733930534142106\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 30.35084012330492\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 31.007363160540557\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 23.890455902121165\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 12.799804690480187\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.429481637893296\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.467407576039792\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.844781697717877\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 25.488301996864326\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.140264702817245\n",
      "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 28.38073714196783\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.31572797382566\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.729621812588453\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.33489998601203\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 16.302107382445993\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 15.021233772047632\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 26.718545556468616\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 14.193441846299617\n",
      "TIMESTEP 101 / STATE explore / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "fps: 29.0454208649285\n",
      "TIMESTEP 102 / STATE explore / EPSILON 0.099999001 / ACTION 0 / REWARD 0.1 / Q_MAX  2.5928195 / Loss  0.9719763994216919\n",
      "fps: 0.6165118541315794\n",
      "TIMESTEP 103 / STATE explore / EPSILON 0.099998002 / ACTION 1 / REWARD -1 / Q_MAX  8.288421 / Loss  0.4042482078075409\n",
      "fps: 7.41769152138312\n",
      "TIMESTEP 104 / STATE explore / EPSILON 0.099997003 / ACTION 1 / REWARD 0.1 / Q_MAX  9.696884 / Loss  0.16315647959709167\n",
      "fps: 6.3222929840296045\n",
      "TIMESTEP 105 / STATE explore / EPSILON 0.099996004 / ACTION 1 / REWARD 0.1 / Q_MAX  9.221404 / Loss  0.13247311115264893\n",
      "----------Random Action----------\n",
      "fps: 6.852318248652181\n",
      "TIMESTEP 106 / STATE explore / EPSILON 0.099995005 / ACTION 1 / REWARD 0.1 / Q_MAX  9.261265 / Loss  0.19079789519309998\n",
      "fps: 7.4206443143478396\n",
      "TIMESTEP 107 / STATE explore / EPSILON 0.099994006 / ACTION 0 / REWARD 0.1 / Q_MAX  8.76067 / Loss  0.15116524696350098\n",
      "----------Random Action----------\n",
      "fps: 8.44855897448293\n",
      "TIMESTEP 108 / STATE explore / EPSILON 0.099993007 / ACTION 0 / REWARD 0.1 / Q_MAX  8.952243 / Loss  0.27531129121780396\n",
      "fps: 8.63197798737608\n",
      "TIMESTEP 109 / STATE explore / EPSILON 0.099992008 / ACTION 0 / REWARD 0.1 / Q_MAX  6.450937 / Loss  0.36575502157211304\n",
      "fps: 8.338477075820169\n",
      "TIMESTEP 110 / STATE explore / EPSILON 0.09999100899999999 / ACTION 0 / REWARD 0.1 / Q_MAX  12.431093 / Loss  0.31677788496017456\n",
      "fps: 8.848126607219328\n",
      "TIMESTEP 111 / STATE explore / EPSILON 0.09999000999999999 / ACTION 0 / REWARD 0.1 / Q_MAX  9.221462 / Loss  0.18691398203372955\n",
      "fps: 8.397559798745462\n",
      "TIMESTEP 112 / STATE explore / EPSILON 0.09998901099999999 / ACTION 0 / REWARD 0.1 / Q_MAX  9.171145 / Loss  0.11733783781528473\n",
      "fps: 7.149621492993231\n",
      "TIMESTEP 113 / STATE explore / EPSILON 0.09998801199999999 / ACTION 1 / REWARD 0.1 / Q_MAX  12.115675 / Loss  0.17791631817817688\n",
      "fps: 7.671337905807041\n",
      "TIMESTEP 114 / STATE explore / EPSILON 0.09998701299999999 / ACTION 0 / REWARD 0.1 / Q_MAX  9.030752 / Loss  0.2237292230129242\n",
      "fps: 7.0024341461622965\n",
      "TIMESTEP 115 / STATE explore / EPSILON 0.09998601399999998 / ACTION 1 / REWARD 0.1 / Q_MAX  7.5715394 / Loss  0.04619491100311279\n",
      "fps: 6.3944481796048045\n",
      "TIMESTEP 116 / STATE explore / EPSILON 0.09998501499999998 / ACTION 1 / REWARD 0.1 / Q_MAX  7.614025 / Loss  0.14773033559322357\n",
      "fps: 7.486726927415785\n",
      "TIMESTEP 117 / STATE explore / EPSILON 0.09998401599999998 / ACTION 1 / REWARD 0.1 / Q_MAX  6.3141932 / Loss  0.0814676508307457\n",
      "fps: 8.273147947248203\n",
      "TIMESTEP 118 / STATE explore / EPSILON 0.09998301699999998 / ACTION 0 / REWARD 0.1 / Q_MAX  8.800598 / Loss  0.1385093331336975\n",
      "fps: 7.09212499767502\n",
      "TIMESTEP 119 / STATE explore / EPSILON 0.09998201799999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.731149 / Loss  0.1339234560728073\n",
      "fps: 5.1791313667890355\n",
      "TIMESTEP 120 / STATE explore / EPSILON 0.09998101899999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.679053 / Loss  0.1331901103258133\n",
      "fps: 7.215336553679314\n",
      "TIMESTEP 121 / STATE explore / EPSILON 0.09998001999999998 / ACTION 1 / REWARD 0.1 / Q_MAX  12.209766 / Loss  1.4886069297790527\n",
      "fps: 6.479150511157728\n",
      "TIMESTEP 122 / STATE explore / EPSILON 0.09997902099999997 / ACTION 1 / REWARD 0.1 / Q_MAX  12.056173 / Loss  0.07468240708112717\n",
      "fps: 7.45122854640514\n",
      "TIMESTEP 123 / STATE explore / EPSILON 0.09997802199999997 / ACTION 1 / REWARD 0.1 / Q_MAX  8.517446 / Loss  0.03326072171330452\n",
      "fps: 6.6585026281240225\n",
      "TIMESTEP 124 / STATE explore / EPSILON 0.09997702299999997 / ACTION 1 / REWARD 0.1 / Q_MAX  8.47601 / Loss  0.041900478303432465\n",
      "fps: 7.43586543370965\n",
      "TIMESTEP 125 / STATE explore / EPSILON 0.09997602399999997 / ACTION 1 / REWARD 0.1 / Q_MAX  12.144843 / Loss  0.04846374690532684\n",
      "fps: 6.677849741757577\n",
      "TIMESTEP 126 / STATE explore / EPSILON 0.09997502499999997 / ACTION 1 / REWARD 0.1 / Q_MAX  8.411787 / Loss  1.219345211982727\n",
      "fps: 7.506503733293244\n",
      "TIMESTEP 127 / STATE explore / EPSILON 0.09997402599999997 / ACTION 1 / REWARD 0.1 / Q_MAX  11.626053 / Loss  0.15352723002433777\n",
      "fps: 6.656611056093039\n",
      "TIMESTEP 128 / STATE explore / EPSILON 0.09997302699999996 / ACTION 1 / REWARD 0.1 / Q_MAX  8.332761 / Loss  0.03712891414761543\n",
      "fps: 6.817224189802211\n",
      "TIMESTEP 129 / STATE explore / EPSILON 0.09997202799999996 / ACTION 1 / REWARD 0.1 / Q_MAX  5.7916384 / Loss  0.04711893945932388\n",
      "fps: 8.104152255820694\n",
      "TIMESTEP 130 / STATE explore / EPSILON 0.09997102899999996 / ACTION 0 / REWARD 0.1 / Q_MAX  8.271282 / Loss  0.09532585740089417\n",
      "fps: 9.461270028174225\n",
      "TIMESTEP 131 / STATE explore / EPSILON 0.09997002999999996 / ACTION 0 / REWARD 0.1 / Q_MAX  11.588047 / Loss  0.09103124588727951\n",
      "fps: 8.658511108266895\n",
      "TIMESTEP 132 / STATE explore / EPSILON 0.09996903099999996 / ACTION 0 / REWARD 0.1 / Q_MAX  11.537126 / Loss  0.12923257052898407\n",
      "----------Random Action----------\n",
      "fps: 9.749139399799638\n",
      "TIMESTEP 133 / STATE explore / EPSILON 0.09996803199999996 / ACTION 0 / REWARD 0.1 / Q_MAX  5.3302584 / Loss  0.06585680693387985\n",
      "fps: 8.066108836499094\n",
      "TIMESTEP 134 / STATE explore / EPSILON 0.09996703299999996 / ACTION 0 / REWARD 0.1 / Q_MAX  8.683015 / Loss  0.02137698046863079\n",
      "fps: 9.243665550778074\n",
      "TIMESTEP 135 / STATE explore / EPSILON 0.09996603399999995 / ACTION 0 / REWARD 0.1 / Q_MAX  11.6506815 / Loss  0.02159874141216278\n",
      "fps: 8.62393802070911\n",
      "TIMESTEP 136 / STATE explore / EPSILON 0.09996503499999995 / ACTION 0 / REWARD 0.1 / Q_MAX  11.440252 / Loss  0.0106168482452631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 7.057636553013412\n",
      "TIMESTEP 137 / STATE explore / EPSILON 0.09996403599999995 / ACTION 1 / REWARD 0.1 / Q_MAX  4.8186393 / Loss  0.04314285144209862\n",
      "fps: 7.025211210044286\n",
      "TIMESTEP 138 / STATE explore / EPSILON 0.09996303699999995 / ACTION 1 / REWARD 0.1 / Q_MAX  6.399618 / Loss  0.8607652187347412\n",
      "fps: 8.947275907675243\n",
      "TIMESTEP 139 / STATE explore / EPSILON 0.09996203799999995 / ACTION 0 / REWARD 0.1 / Q_MAX  8.169534 / Loss  0.8525620102882385\n",
      "fps: 8.835061318548913\n",
      "TIMESTEP 140 / STATE explore / EPSILON 0.09996103899999995 / ACTION 0 / REWARD 0.1 / Q_MAX  4.4245343 / Loss  0.043837837874889374\n",
      "fps: 7.899294124737274\n",
      "TIMESTEP 141 / STATE explore / EPSILON 0.09996003999999994 / ACTION 0 / REWARD 0.1 / Q_MAX  11.022433 / Loss  0.777346670627594\n",
      "fps: 9.134355011879899\n",
      "TIMESTEP 142 / STATE explore / EPSILON 0.09995904099999994 / ACTION 0 / REWARD 0.1 / Q_MAX  5.7934656 / Loss  0.1268974244594574\n",
      "fps: 6.66828406675432\n",
      "TIMESTEP 143 / STATE explore / EPSILON 0.09995804199999994 / ACTION 1 / REWARD 0.1 / Q_MAX  4.1364527 / Loss  0.7022056579589844\n",
      "fps: 8.915098029213215\n",
      "TIMESTEP 144 / STATE explore / EPSILON 0.09995704299999994 / ACTION 0 / REWARD 0.1 / Q_MAX  7.8137083 / Loss  0.13175716996192932\n",
      "fps: 7.659486190549237\n",
      "TIMESTEP 145 / STATE explore / EPSILON 0.09995604399999994 / ACTION 0 / REWARD 0.1 / Q_MAX  6.1940174 / Loss  0.06037285551428795\n",
      "fps: 7.125245050573002\n",
      "TIMESTEP 146 / STATE explore / EPSILON 0.09995504499999994 / ACTION 1 / REWARD 0.1 / Q_MAX  7.9932427 / Loss  0.06755159795284271\n",
      "----------Random Action----------\n",
      "fps: 7.882548393159181\n",
      "TIMESTEP 147 / STATE explore / EPSILON 0.09995404599999994 / ACTION 0 / REWARD 0.1 / Q_MAX  8.264628 / Loss  0.03281044587492943\n",
      "fps: 7.780470838334128\n",
      "TIMESTEP 148 / STATE explore / EPSILON 0.09995304699999993 / ACTION 0 / REWARD -1 / Q_MAX  4.626271 / Loss  0.08074593544006348\n",
      "fps: 5.8171892835001575\n",
      "TIMESTEP 149 / STATE explore / EPSILON 0.09995204799999993 / ACTION 0 / REWARD 0.1 / Q_MAX  7.860257 / Loss  0.02769279107451439\n",
      "fps: 6.803942561626858\n",
      "TIMESTEP 150 / STATE explore / EPSILON 0.09995104899999993 / ACTION 0 / REWARD 0.1 / Q_MAX  7.878427 / Loss  0.04559333249926567\n",
      "fps: 4.902178588125293\n",
      "TIMESTEP 151 / STATE explore / EPSILON 0.09995004999999993 / ACTION 0 / REWARD 0.1 / Q_MAX  7.6003804 / Loss  0.02102992683649063\n",
      "fps: 5.863510609892314\n",
      "TIMESTEP 152 / STATE explore / EPSILON 0.09994905099999993 / ACTION 0 / REWARD 0.1 / Q_MAX  7.7204533 / Loss  0.014858577400445938\n",
      "fps: 6.1512505462982245\n",
      "TIMESTEP 153 / STATE explore / EPSILON 0.09994805199999993 / ACTION 0 / REWARD 0.1 / Q_MAX  8.427297 / Loss  0.0285550095140934\n",
      "fps: 6.664872122863748\n",
      "TIMESTEP 154 / STATE explore / EPSILON 0.09994705299999992 / ACTION 0 / REWARD 0.1 / Q_MAX  5.430488 / Loss  0.014951047487556934\n",
      "fps: 8.223287037400109\n",
      "TIMESTEP 155 / STATE explore / EPSILON 0.09994605399999992 / ACTION 0 / REWARD 0.1 / Q_MAX  11.547771 / Loss  0.0319749154150486\n",
      "fps: 6.202160395702868\n",
      "TIMESTEP 156 / STATE explore / EPSILON 0.09994505499999992 / ACTION 1 / REWARD 0.1 / Q_MAX  11.508196 / Loss  0.01185774989426136\n",
      "fps: 9.398853127233858\n",
      "TIMESTEP 157 / STATE explore / EPSILON 0.09994405599999992 / ACTION 0 / REWARD 0.1 / Q_MAX  11.677911 / Loss  0.048753149807453156\n",
      "fps: 7.526574382790265\n",
      "TIMESTEP 158 / STATE explore / EPSILON 0.09994305699999992 / ACTION 0 / REWARD 0.1 / Q_MAX  7.845363 / Loss  0.027311764657497406\n",
      "fps: 4.661327014955396\n",
      "TIMESTEP 159 / STATE explore / EPSILON 0.09994205799999992 / ACTION 1 / REWARD 0.1 / Q_MAX  7.8520784 / Loss  0.5133699774742126\n",
      "fps: 9.136006412602157\n",
      "TIMESTEP 160 / STATE explore / EPSILON 0.09994105899999992 / ACTION 0 / REWARD 0.1 / Q_MAX  12.077031 / Loss  0.028145886957645416\n",
      "fps: 5.298936120484576\n",
      "TIMESTEP 161 / STATE explore / EPSILON 0.09994005999999991 / ACTION 1 / REWARD 0.1 / Q_MAX  6.5714383 / Loss  0.017687099054455757\n",
      "fps: 6.057613929480273\n",
      "TIMESTEP 162 / STATE explore / EPSILON 0.09993906099999991 / ACTION 1 / REWARD 0.1 / Q_MAX  11.542451 / Loss  0.05361700803041458\n",
      "fps: 6.7400898613512625\n",
      "TIMESTEP 163 / STATE explore / EPSILON 0.09993806199999991 / ACTION 1 / REWARD 0.1 / Q_MAX  8.216334 / Loss  3.4739315509796143\n",
      "fps: 7.46992655282711\n",
      "TIMESTEP 164 / STATE explore / EPSILON 0.09993706299999991 / ACTION 1 / REWARD 0.1 / Q_MAX  0.7837577 / Loss  3.457580327987671\n",
      "fps: 6.332248343830959\n",
      "TIMESTEP 165 / STATE explore / EPSILON 0.09993606399999991 / ACTION 1 / REWARD 0.1 / Q_MAX  8.205205 / Loss  0.014371780678629875\n",
      "----------Random Action----------\n",
      "fps: 9.887142931228755\n",
      "TIMESTEP 166 / STATE explore / EPSILON 0.0999350649999999 / ACTION 0 / REWARD 0.1 / Q_MAX  7.677609 / Loss  0.016686666756868362\n",
      "fps: 8.49771870821346\n",
      "TIMESTEP 167 / STATE explore / EPSILON 0.0999340659999999 / ACTION 0 / REWARD 0.1 / Q_MAX  0.048177898 / Loss  2.4021170139312744\n",
      "fps: 5.6567718546770855\n",
      "TIMESTEP 168 / STATE explore / EPSILON 0.0999330669999999 / ACTION 0 / REWARD 0.1 / Q_MAX  8.191422 / Loss  0.016691405326128006\n",
      "fps: 3.676363241135821\n",
      "TIMESTEP 169 / STATE explore / EPSILON 0.0999320679999999 / ACTION 0 / REWARD 0.1 / Q_MAX  8.188105 / Loss  0.4042295813560486\n",
      "fps: 3.1542760881990195\n",
      "TIMESTEP 170 / STATE explore / EPSILON 0.0999310689999999 / ACTION 0 / REWARD 0.1 / Q_MAX  5.4065104 / Loss  0.021395660936832428\n",
      "----------Random Action----------\n",
      "fps: 3.065603845102479\n",
      "TIMESTEP 171 / STATE explore / EPSILON 0.0999300699999999 / ACTION 0 / REWARD 0.1 / Q_MAX  5.4900527 / Loss  0.041827961802482605\n",
      "fps: 3.2490334554924747\n",
      "TIMESTEP 172 / STATE explore / EPSILON 0.0999290709999999 / ACTION 0 / REWARD 0.1 / Q_MAX  9.012945 / Loss  0.04005199670791626\n",
      "fps: 3.2390999136611534\n",
      "TIMESTEP 173 / STATE explore / EPSILON 0.0999280719999999 / ACTION 0 / REWARD 0.1 / Q_MAX  11.030862 / Loss  0.018423009663820267\n",
      "fps: 3.770994754795218\n",
      "TIMESTEP 174 / STATE explore / EPSILON 0.0999270729999999 / ACTION 1 / REWARD -1 / Q_MAX  6.0358634 / Loss  0.11099135130643845\n",
      "fps: 8.73431207180192\n",
      "TIMESTEP 175 / STATE explore / EPSILON 0.09992607399999989 / ACTION 0 / REWARD 0.1 / Q_MAX  8.227655 / Loss  0.01932797022163868\n",
      "fps: 7.0619738419471725\n",
      "TIMESTEP 176 / STATE explore / EPSILON 0.09992507499999989 / ACTION 0 / REWARD 0.1 / Q_MAX  10.872564 / Loss  3.130472421646118\n",
      "fps: 8.392350988540889\n",
      "TIMESTEP 177 / STATE explore / EPSILON 0.09992407599999989 / ACTION 0 / REWARD 0.1 / Q_MAX  8.211186 / Loss  0.03910959139466286\n",
      "fps: 6.800919695294183\n",
      "TIMESTEP 178 / STATE explore / EPSILON 0.09992307699999989 / ACTION 0 / REWARD 0.1 / Q_MAX  8.645041 / Loss  0.16961421072483063\n",
      "fps: 8.076779689738574\n",
      "TIMESTEP 179 / STATE explore / EPSILON 0.09992207799999989 / ACTION 0 / REWARD 0.1 / Q_MAX  10.485368 / Loss  0.03688529506325722\n",
      "fps: 6.318311985092636\n",
      "TIMESTEP 180 / STATE explore / EPSILON 0.09992107899999988 / ACTION 0 / REWARD 0.1 / Q_MAX  7.8691416 / Loss  0.019965872168540955\n",
      "fps: 8.182060075455208\n",
      "TIMESTEP 181 / STATE explore / EPSILON 0.09992007999999988 / ACTION 0 / REWARD 0.1 / Q_MAX  8.1121 / Loss  0.4243081510066986\n",
      "fps: 5.9924449803695214\n",
      "TIMESTEP 182 / STATE explore / EPSILON 0.09991908099999988 / ACTION 0 / REWARD 0.1 / Q_MAX  10.396575 / Loss  0.033139318227767944\n",
      "fps: 8.126732182162012\n",
      "TIMESTEP 183 / STATE explore / EPSILON 0.09991808199999988 / ACTION 0 / REWARD 0.1 / Q_MAX  10.655715 / Loss  0.027006519958376884\n",
      "fps: 6.063533826977734\n",
      "TIMESTEP 184 / STATE explore / EPSILON 0.09991708299999988 / ACTION 0 / REWARD 0.1 / Q_MAX  5.9554963 / Loss  0.04939776286482811\n",
      "fps: 8.897679855237563\n",
      "TIMESTEP 185 / STATE explore / EPSILON 0.09991608399999988 / ACTION 0 / REWARD 0.1 / Q_MAX  8.046903 / Loss  0.025211120024323463\n",
      "fps: 7.07414666068483\n",
      "TIMESTEP 186 / STATE explore / EPSILON 0.09991508499999988 / ACTION 0 / REWARD 0.1 / Q_MAX  10.383613 / Loss  0.20899221301078796\n",
      "fps: 3.9259790367540104\n",
      "TIMESTEP 187 / STATE explore / EPSILON 0.09991408599999987 / ACTION 0 / REWARD 0.1 / Q_MAX  10.486635 / Loss  3.0139594078063965\n",
      "fps: 7.141331589844175\n",
      "TIMESTEP 188 / STATE explore / EPSILON 0.09991308699999987 / ACTION 1 / REWARD 0.1 / Q_MAX  9.985137 / Loss  0.0814509317278862\n",
      "fps: 6.532980280988131\n",
      "TIMESTEP 189 / STATE explore / EPSILON 0.09991208799999987 / ACTION 1 / REWARD 0.1 / Q_MAX  7.7221274 / Loss  0.031193343922495842\n",
      "fps: 9.425783514838802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 190 / STATE explore / EPSILON 0.09991108899999987 / ACTION 0 / REWARD 0.1 / Q_MAX  10.327347 / Loss  0.09079746901988983\n",
      "fps: 6.105734811375001\n",
      "TIMESTEP 191 / STATE explore / EPSILON 0.09991008999999987 / ACTION 0 / REWARD 0.1 / Q_MAX  5.26234 / Loss  0.028471512719988823\n",
      "fps: 6.7509600991485454\n",
      "TIMESTEP 192 / STATE explore / EPSILON 0.09990909099999987 / ACTION 0 / REWARD 0.1 / Q_MAX  9.922419 / Loss  0.9388775825500488\n",
      "fps: 6.3051760631133345\n",
      "TIMESTEP 193 / STATE explore / EPSILON 0.09990809199999987 / ACTION 0 / REWARD 0.1 / Q_MAX  7.9722 / Loss  0.014611373655498028\n",
      "fps: 4.957009275104712\n",
      "TIMESTEP 194 / STATE explore / EPSILON 0.09990709299999986 / ACTION 1 / REWARD 0.1 / Q_MAX  10.1039095 / Loss  0.030314577743411064\n",
      "----------Random Action----------\n",
      "fps: 5.607148395514606\n",
      "TIMESTEP 195 / STATE explore / EPSILON 0.09990609399999986 / ACTION 1 / REWARD 0.1 / Q_MAX  10.067735 / Loss  0.022336360067129135\n",
      "fps: 6.778826510822063\n",
      "TIMESTEP 196 / STATE explore / EPSILON 0.09990509499999986 / ACTION 0 / REWARD 0.1 / Q_MAX  7.9476132 / Loss  0.02144141122698784\n",
      "fps: 7.155012325039875\n",
      "TIMESTEP 197 / STATE explore / EPSILON 0.09990409599999986 / ACTION 0 / REWARD 0.1 / Q_MAX  7.947897 / Loss  3.0319151878356934\n",
      "fps: 4.952900082424265\n",
      "TIMESTEP 198 / STATE explore / EPSILON 0.09990309699999986 / ACTION 1 / REWARD 0.1 / Q_MAX  9.9938545 / Loss  0.019781971350312233\n",
      "----------Random Action----------\n",
      "fps: 4.798217215130792\n",
      "TIMESTEP 199 / STATE explore / EPSILON 0.09990209799999986 / ACTION 1 / REWARD 0.1 / Q_MAX  10.119303 / Loss  0.04992585629224777\n",
      "fps: 6.403449123444482\n",
      "TIMESTEP 200 / STATE explore / EPSILON 0.09990109899999985 / ACTION 1 / REWARD 0.1 / Q_MAX  8.864096 / Loss  0.06370889395475388\n",
      "fps: 8.579326426460211\n",
      "TIMESTEP 201 / STATE explore / EPSILON 0.09990009999999985 / ACTION 0 / REWARD 0.1 / Q_MAX  8.496951 / Loss  0.012423857115209103\n",
      "fps: 9.075338894117902\n",
      "TIMESTEP 202 / STATE explore / EPSILON 0.09989910099999985 / ACTION 0 / REWARD 0.1 / Q_MAX  9.78942 / Loss  0.03154303878545761\n",
      "fps: 9.132465586991643\n",
      "TIMESTEP 203 / STATE explore / EPSILON 0.09989810199999985 / ACTION 0 / REWARD 0.1 / Q_MAX  4.973701 / Loss  0.07273362576961517\n",
      "fps: 7.081037734097098\n",
      "TIMESTEP 204 / STATE explore / EPSILON 0.09989710299999985 / ACTION 1 / REWARD 0.1 / Q_MAX  8.567409 / Loss  0.05345674231648445\n",
      "fps: 6.632663420681847\n",
      "TIMESTEP 205 / STATE explore / EPSILON 0.09989610399999985 / ACTION 1 / REWARD 0.1 / Q_MAX  5.7549834 / Loss  0.06038401275873184\n",
      "fps: 7.368886718166661\n",
      "TIMESTEP 206 / STATE explore / EPSILON 0.09989510499999985 / ACTION 1 / REWARD 0.1 / Q_MAX  7.9390383 / Loss  0.034105148166418076\n",
      "fps: 6.508013399887973\n",
      "TIMESTEP 207 / STATE explore / EPSILON 0.09989410599999984 / ACTION 1 / REWARD 0.1 / Q_MAX  8.060496 / Loss  0.06805402040481567\n",
      "fps: 9.469472533972713\n",
      "TIMESTEP 208 / STATE explore / EPSILON 0.09989310699999984 / ACTION 0 / REWARD 0.1 / Q_MAX  8.30265 / Loss  0.07545692473649979\n",
      "fps: 8.038237312042684\n",
      "TIMESTEP 209 / STATE explore / EPSILON 0.09989210799999984 / ACTION 0 / REWARD 0.1 / Q_MAX  8.260824 / Loss  0.08409561216831207\n",
      "fps: 9.194123552699072\n",
      "TIMESTEP 210 / STATE explore / EPSILON 0.09989110899999984 / ACTION 0 / REWARD 0.1 / Q_MAX  8.474055 / Loss  0.015396475791931152\n",
      "----------Random Action----------\n",
      "fps: 8.95623634721371\n",
      "TIMESTEP 211 / STATE explore / EPSILON 0.09989010999999984 / ACTION 0 / REWARD 0.1 / Q_MAX  0.78187025 / Loss  0.09313827753067017\n",
      "fps: 9.34128866587826\n",
      "TIMESTEP 212 / STATE explore / EPSILON 0.09988911099999984 / ACTION 0 / REWARD 0.1 / Q_MAX  8.338217 / Loss  0.03799990564584732\n",
      "fps: 6.663643830697884\n",
      "TIMESTEP 213 / STATE explore / EPSILON 0.09988811199999983 / ACTION 1 / REWARD 0.1 / Q_MAX  10.20855 / Loss  0.05760396271944046\n",
      "fps: 6.973816000372442\n",
      "TIMESTEP 214 / STATE explore / EPSILON 0.09988711299999983 / ACTION 1 / REWARD 0.1 / Q_MAX  10.33239 / Loss  0.0981774777173996\n",
      "fps: 7.8401576142525755\n",
      "TIMESTEP 215 / STATE explore / EPSILON 0.09988611399999983 / ACTION 0 / REWARD 0.1 / Q_MAX  10.336643 / Loss  0.06675488501787186\n",
      "fps: 8.921754186688108\n",
      "TIMESTEP 216 / STATE explore / EPSILON 0.09988511499999983 / ACTION 0 / REWARD 0.1 / Q_MAX  8.397757 / Loss  0.0581447035074234\n",
      "fps: 9.206636024004881\n",
      "TIMESTEP 217 / STATE explore / EPSILON 0.09988411599999983 / ACTION 0 / REWARD 0.1 / Q_MAX  9.001696 / Loss  0.045063316822052\n",
      "fps: 8.553450090137122\n",
      "TIMESTEP 218 / STATE explore / EPSILON 0.09988311699999983 / ACTION 0 / REWARD 0.1 / Q_MAX  8.507595 / Loss  0.3024871051311493\n",
      "fps: 8.519293969492008\n",
      "TIMESTEP 219 / STATE explore / EPSILON 0.09988211799999983 / ACTION 0 / REWARD 0.1 / Q_MAX  8.539861 / Loss  0.03267243131995201\n",
      "fps: 8.099379169844841\n",
      "TIMESTEP 220 / STATE explore / EPSILON 0.09988111899999982 / ACTION 0 / REWARD 0.1 / Q_MAX  5.8852377 / Loss  0.08707393705844879\n",
      "fps: 7.605348380943401\n",
      "TIMESTEP 221 / STATE explore / EPSILON 0.09988011999999982 / ACTION 1 / REWARD 0.1 / Q_MAX  8.577086 / Loss  0.07197483628988266\n",
      "fps: 6.52051855663532\n",
      "TIMESTEP 222 / STATE explore / EPSILON 0.09987912099999982 / ACTION 1 / REWARD 0.1 / Q_MAX  10.226314 / Loss  0.11856085062026978\n",
      "fps: 8.687672435934614\n",
      "TIMESTEP 223 / STATE explore / EPSILON 0.09987812199999982 / ACTION 0 / REWARD 0.1 / Q_MAX  10.245049 / Loss  0.7544992566108704\n",
      "fps: 8.029189367168089\n",
      "TIMESTEP 224 / STATE explore / EPSILON 0.09987712299999982 / ACTION 0 / REWARD 0.1 / Q_MAX  8.560062 / Loss  0.09495054185390472\n",
      "fps: 9.525062224079356\n",
      "TIMESTEP 225 / STATE explore / EPSILON 0.09987612399999982 / ACTION 0 / REWARD 0.1 / Q_MAX  9.663191 / Loss  0.051969800144433975\n",
      "fps: 8.579098298622617\n",
      "TIMESTEP 226 / STATE explore / EPSILON 0.09987512499999981 / ACTION 0 / REWARD 0.1 / Q_MAX  5.385621 / Loss  0.06021077558398247\n",
      "fps: 9.134394797648836\n",
      "TIMESTEP 227 / STATE explore / EPSILON 0.09987412599999981 / ACTION 0 / REWARD 0.1 / Q_MAX  3.226369 / Loss  1.7552472352981567\n",
      "fps: 8.597809910748328\n",
      "TIMESTEP 228 / STATE explore / EPSILON 0.09987312699999981 / ACTION 0 / REWARD 0.1 / Q_MAX  5.3932304 / Loss  1.2327868938446045\n",
      "fps: 8.996733182755152\n",
      "TIMESTEP 229 / STATE explore / EPSILON 0.09987212799999981 / ACTION 0 / REWARD 0.1 / Q_MAX  8.987041 / Loss  0.07448748499155045\n",
      "fps: 8.245596376446905\n",
      "TIMESTEP 230 / STATE explore / EPSILON 0.09987112899999981 / ACTION 0 / REWARD 0.1 / Q_MAX  8.6747675 / Loss  1.4002695083618164\n",
      "fps: 6.743622641752122\n",
      "TIMESTEP 231 / STATE explore / EPSILON 0.09987012999999981 / ACTION 1 / REWARD 0.1 / Q_MAX  8.399393 / Loss  0.12706950306892395\n",
      "fps: 6.556201992665839\n",
      "TIMESTEP 232 / STATE explore / EPSILON 0.0998691309999998 / ACTION 1 / REWARD 0.1 / Q_MAX  10.117962 / Loss  0.056906986981630325\n",
      "fps: 7.089200148061259\n",
      "TIMESTEP 233 / STATE explore / EPSILON 0.0998681319999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.983125 / Loss  0.3658544719219208\n",
      "----------Random Action----------\n",
      "fps: 6.522455276056829\n",
      "TIMESTEP 234 / STATE explore / EPSILON 0.0998671329999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.301613 / Loss  0.056017275899648666\n",
      "fps: 4.379865773213747\n",
      "TIMESTEP 235 / STATE explore / EPSILON 0.0998661339999998 / ACTION 1 / REWARD 0.1 / Q_MAX  9.719634 / Loss  0.28379935026168823\n",
      "fps: 9.262179744811656\n",
      "TIMESTEP 236 / STATE explore / EPSILON 0.0998651349999998 / ACTION 0 / REWARD 0.1 / Q_MAX  8.84634 / Loss  0.043787017464637756\n",
      "----------Random Action----------\n",
      "fps: 6.816016640665627\n",
      "TIMESTEP 237 / STATE explore / EPSILON 0.0998641359999998 / ACTION 0 / REWARD 0.1 / Q_MAX  9.561024 / Loss  1.1285167932510376\n",
      "fps: 7.124059026551078\n",
      "TIMESTEP 238 / STATE explore / EPSILON 0.0998631369999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.151591 / Loss  0.04847251623868942\n",
      "fps: 6.496592401675299\n",
      "TIMESTEP 239 / STATE explore / EPSILON 0.0998621379999998 / ACTION 1 / REWARD 0.1 / Q_MAX  8.792359 / Loss  0.06224075332283974\n",
      "fps: 7.567981494613121\n",
      "TIMESTEP 240 / STATE explore / EPSILON 0.0998611389999998 / ACTION 1 / REWARD 0.1 / Q_MAX  5.3475685 / Loss  0.04811447113752365\n",
      "fps: 6.558283023971805\n",
      "TIMESTEP 241 / STATE explore / EPSILON 0.09986013999999979 / ACTION 1 / REWARD 0.1 / Q_MAX  9.956653 / Loss  0.45129990577697754\n",
      "fps: 7.452724649423939\n",
      "TIMESTEP 242 / STATE explore / EPSILON 0.09985914099999979 / ACTION 1 / REWARD 0.1 / Q_MAX  4.2501187 / Loss  0.8250986337661743\n",
      "fps: 6.232397798461776\n",
      "TIMESTEP 243 / STATE explore / EPSILON 0.09985814199999979 / ACTION 1 / REWARD 0.1 / Q_MAX  6.205188 / Loss  0.12027055770158768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 7.179212260945622\n",
      "TIMESTEP 244 / STATE explore / EPSILON 0.09985714299999979 / ACTION 1 / REWARD 0.1 / Q_MAX  5.3881197 / Loss  0.09840026497840881\n",
      "----------Random Action----------\n",
      "fps: 7.989256999617138\n",
      "TIMESTEP 245 / STATE explore / EPSILON 0.09985614399999979 / ACTION 0 / REWARD 0.1 / Q_MAX  5.3509636 / Loss  0.39882755279541016\n",
      "fps: 7.050020590484674\n",
      "TIMESTEP 246 / STATE explore / EPSILON 0.09985514499999978 / ACTION 1 / REWARD 0.1 / Q_MAX  6.2148285 / Loss  0.05140404775738716\n",
      "fps: 6.772521766088821\n",
      "TIMESTEP 247 / STATE explore / EPSILON 0.09985414599999978 / ACTION 1 / REWARD 0.1 / Q_MAX  9.23596 / Loss  0.07452288269996643\n",
      "fps: 6.812728516040588\n",
      "TIMESTEP 248 / STATE explore / EPSILON 0.09985314699999978 / ACTION 1 / REWARD 0.1 / Q_MAX  10.003293 / Loss  0.0872301533818245\n",
      "fps: 9.314817570283576\n",
      "TIMESTEP 249 / STATE explore / EPSILON 0.09985214799999978 / ACTION 0 / REWARD 0.1 / Q_MAX  8.5724325 / Loss  0.06086870655417442\n",
      "fps: 6.951070924289531\n",
      "TIMESTEP 250 / STATE explore / EPSILON 0.09985114899999978 / ACTION 1 / REWARD 0.1 / Q_MAX  9.424083 / Loss  0.10430929064750671\n",
      "fps: 7.568240953594203\n",
      "TIMESTEP 251 / STATE explore / EPSILON 0.09985014999999978 / ACTION 1 / REWARD 0.1 / Q_MAX  8.83479 / Loss  0.04600551351904869\n",
      "fps: 6.318140667107529\n",
      "TIMESTEP 252 / STATE explore / EPSILON 0.09984915099999978 / ACTION 1 / REWARD 0.1 / Q_MAX  8.719318 / Loss  0.2365046888589859\n",
      "----------Random Action----------\n",
      "fps: 9.850547447826884\n",
      "TIMESTEP 253 / STATE explore / EPSILON 0.09984815199999977 / ACTION 0 / REWARD 0.1 / Q_MAX  8.846541 / Loss  0.07510087639093399\n",
      "fps: 6.68181781975693\n",
      "TIMESTEP 254 / STATE explore / EPSILON 0.09984715299999977 / ACTION 1 / REWARD -1 / Q_MAX  9.053813 / Loss  2.0258264541625977\n",
      "fps: 6.903647277832735\n",
      "TIMESTEP 255 / STATE explore / EPSILON 0.09984615399999977 / ACTION 1 / REWARD 0.1 / Q_MAX  7.5165505 / Loss  1.9991875886917114\n",
      "----------Random Action----------\n",
      "fps: 7.1245188665464045\n",
      "TIMESTEP 256 / STATE explore / EPSILON 0.09984515499999977 / ACTION 1 / REWARD 0.1 / Q_MAX  7.50538 / Loss  1.8397575616836548\n",
      "fps: 7.05546397163216\n",
      "TIMESTEP 257 / STATE explore / EPSILON 0.09984415599999977 / ACTION 1 / REWARD 0.1 / Q_MAX  7.151949 / Loss  0.45077282190322876\n",
      "fps: 9.098312790944863\n",
      "TIMESTEP 258 / STATE explore / EPSILON 0.09984315699999977 / ACTION 0 / REWARD 0.1 / Q_MAX  5.345423 / Loss  0.26060348749160767\n",
      "fps: 7.942749688959924\n",
      "TIMESTEP 259 / STATE explore / EPSILON 0.09984215799999976 / ACTION 0 / REWARD 0.1 / Q_MAX  4.83407 / Loss  0.9039010405540466\n",
      "----------Random Action----------\n",
      "fps: 9.79586193366669\n",
      "TIMESTEP 260 / STATE explore / EPSILON 0.09984115899999976 / ACTION 0 / REWARD 0.1 / Q_MAX  8.2072735 / Loss  0.056319672614336014\n",
      "fps: 8.783238924895558\n",
      "TIMESTEP 261 / STATE explore / EPSILON 0.09984015999999976 / ACTION 0 / REWARD 0.1 / Q_MAX  8.950982 / Loss  0.04167594760656357\n",
      "fps: 8.770712877208464\n",
      "TIMESTEP 262 / STATE explore / EPSILON 0.09983916099999976 / ACTION 0 / REWARD 0.1 / Q_MAX  0.58452725 / Loss  0.04277826100587845\n",
      "fps: 8.579115846487092\n",
      "TIMESTEP 263 / STATE explore / EPSILON 0.09983816199999976 / ACTION 0 / REWARD 0.1 / Q_MAX  7.271513 / Loss  0.07891736924648285\n",
      "fps: 9.08567950281497\n",
      "TIMESTEP 264 / STATE explore / EPSILON 0.09983716299999976 / ACTION 0 / REWARD 0.1 / Q_MAX  7.270917 / Loss  0.03664419427514076\n",
      "fps: 7.271690831641525\n",
      "TIMESTEP 265 / STATE explore / EPSILON 0.09983616399999976 / ACTION 1 / REWARD 0.1 / Q_MAX  7.933132 / Loss  0.05426066368818283\n",
      "fps: 9.417466180185237\n",
      "TIMESTEP 266 / STATE explore / EPSILON 0.09983516499999975 / ACTION 0 / REWARD 0.1 / Q_MAX  8.902646 / Loss  0.07177384197711945\n",
      "fps: 8.092893665463947\n",
      "TIMESTEP 267 / STATE explore / EPSILON 0.09983416599999975 / ACTION 0 / REWARD 0.1 / Q_MAX  5.4440956 / Loss  0.05699821561574936\n",
      "fps: 9.004053032905132\n",
      "TIMESTEP 268 / STATE explore / EPSILON 0.09983316699999975 / ACTION 0 / REWARD 0.1 / Q_MAX  8.325364 / Loss  0.4945816397666931\n",
      "fps: 8.922304570169244\n",
      "TIMESTEP 269 / STATE explore / EPSILON 0.09983216799999975 / ACTION 0 / REWARD 0.1 / Q_MAX  5.7892737 / Loss  0.04251404106616974\n",
      "fps: 8.423345878402532\n",
      "TIMESTEP 270 / STATE explore / EPSILON 0.09983116899999975 / ACTION 0 / REWARD 0.1 / Q_MAX  7.9418116 / Loss  0.05345018953084946\n",
      "fps: 8.301607156994695\n",
      "TIMESTEP 271 / STATE explore / EPSILON 0.09983016999999975 / ACTION 0 / REWARD 0.1 / Q_MAX  9.270211 / Loss  0.02965538203716278\n",
      "fps: 9.12388352555764\n",
      "TIMESTEP 272 / STATE explore / EPSILON 0.09982917099999974 / ACTION 0 / REWARD 0.1 / Q_MAX  9.47165 / Loss  0.048525139689445496\n",
      "fps: 8.882341854313893\n",
      "TIMESTEP 273 / STATE explore / EPSILON 0.09982817199999974 / ACTION 0 / REWARD 0.1 / Q_MAX  7.5732045 / Loss  0.0797344297170639\n",
      "fps: 9.272602264261064\n",
      "TIMESTEP 274 / STATE explore / EPSILON 0.09982717299999974 / ACTION 0 / REWARD 0.1 / Q_MAX  6.472254 / Loss  1.0680365562438965\n",
      "----------Random Action----------\n",
      "fps: 8.527798730885513\n",
      "TIMESTEP 275 / STATE explore / EPSILON 0.09982617399999974 / ACTION 0 / REWARD 0.1 / Q_MAX  8.541531 / Loss  0.07905946671962738\n",
      "----------Random Action----------\n",
      "fps: 7.32112884532138\n",
      "TIMESTEP 276 / STATE explore / EPSILON 0.09982517499999974 / ACTION 1 / REWARD 0.1 / Q_MAX  8.933945 / Loss  0.197588250041008\n",
      "fps: 7.778998966955561\n",
      "TIMESTEP 277 / STATE explore / EPSILON 0.09982417599999974 / ACTION 0 / REWARD 0.1 / Q_MAX  6.007129 / Loss  0.06916092336177826\n",
      "----------Random Action----------\n",
      "fps: 9.350931012342157\n",
      "TIMESTEP 278 / STATE explore / EPSILON 0.09982317699999974 / ACTION 0 / REWARD 0.1 / Q_MAX  9.4790325 / Loss  0.1108005940914154\n",
      "----------Random Action----------\n",
      "fps: 8.574573346744188\n",
      "TIMESTEP 279 / STATE explore / EPSILON 0.09982217799999973 / ACTION 0 / REWARD 0.1 / Q_MAX  7.4323683 / Loss  1.3874850273132324\n",
      "fps: 9.112386646563223\n",
      "TIMESTEP 280 / STATE explore / EPSILON 0.09982117899999973 / ACTION 0 / REWARD 0.1 / Q_MAX  8.5733385 / Loss  0.16238170862197876\n",
      "fps: 8.351194647977064\n",
      "TIMESTEP 281 / STATE explore / EPSILON 0.09982017999999973 / ACTION 0 / REWARD 0.1 / Q_MAX  9.899903 / Loss  0.0590573251247406\n",
      "fps: 9.305641774289093\n",
      "TIMESTEP 282 / STATE explore / EPSILON 0.09981918099999973 / ACTION 0 / REWARD 0.1 / Q_MAX  5.643029 / Loss  0.12144717574119568\n",
      "fps: 6.444457077710463\n",
      "TIMESTEP 283 / STATE explore / EPSILON 0.09981818199999973 / ACTION 1 / REWARD 0.1 / Q_MAX  9.524057 / Loss  0.02290065959095955\n",
      "fps: 8.683571558706129\n",
      "TIMESTEP 284 / STATE explore / EPSILON 0.09981718299999973 / ACTION 0 / REWARD 0.1 / Q_MAX  8.892642 / Loss  0.20333872735500336\n",
      "fps: 7.565756340416321\n",
      "TIMESTEP 285 / STATE explore / EPSILON 0.09981618399999972 / ACTION 0 / REWARD 0.1 / Q_MAX  9.68867 / Loss  0.5478030443191528\n",
      "fps: 9.636384940391538\n",
      "TIMESTEP 286 / STATE explore / EPSILON 0.09981518499999972 / ACTION 0 / REWARD 0.1 / Q_MAX  8.7191 / Loss  0.039764612913131714\n",
      "fps: 8.623033796799799\n",
      "TIMESTEP 287 / STATE explore / EPSILON 0.09981418599999972 / ACTION 0 / REWARD 0.1 / Q_MAX  6.564841 / Loss  0.11969184875488281\n",
      "fps: 7.716032326250085\n",
      "TIMESTEP 288 / STATE explore / EPSILON 0.09981318699999972 / ACTION 0 / REWARD 0.1 / Q_MAX  8.445963 / Loss  0.043702416121959686\n",
      "fps: 8.218565872106124\n",
      "TIMESTEP 289 / STATE explore / EPSILON 0.09981218799999972 / ACTION 0 / REWARD 0.1 / Q_MAX  6.514952 / Loss  0.729748547077179\n",
      "fps: 11.667762700359967\n",
      "TIMESTEP 290 / STATE explore / EPSILON 0.09981118899999972 / ACTION 0 / REWARD 0.1 / Q_MAX  6.869478 / Loss  0.033908355981111526\n",
      "fps: 9.005193606217729\n",
      "TIMESTEP 291 / STATE explore / EPSILON 0.09981018999999972 / ACTION 1 / REWARD 0.1 / Q_MAX  9.29019 / Loss  0.05318526178598404\n",
      "fps: 7.836729813101749\n",
      "TIMESTEP 292 / STATE explore / EPSILON 0.09980919099999971 / ACTION 0 / REWARD 0.1 / Q_MAX  6.113161 / Loss  0.14692315459251404\n",
      "fps: 8.211872088902638\n",
      "TIMESTEP 293 / STATE explore / EPSILON 0.09980819199999971 / ACTION 0 / REWARD 0.1 / Q_MAX  9.303739 / Loss  0.02577306516468525\n",
      "fps: 4.24437059554505\n",
      "TIMESTEP 294 / STATE explore / EPSILON 0.09980719299999971 / ACTION 1 / REWARD 0.1 / Q_MAX  5.9887853 / Loss  0.1387127786874771\n",
      "fps: 4.706086164279198\n",
      "TIMESTEP 295 / STATE explore / EPSILON 0.09980619399999971 / ACTION 1 / REWARD 0.1 / Q_MAX  0.7412491 / Loss  0.40942975878715515\n",
      "fps: 6.323741864087097\n",
      "TIMESTEP 296 / STATE explore / EPSILON 0.09980519499999971 / ACTION 1 / REWARD 0.1 / Q_MAX  7.9160566 / Loss  0.05585213005542755\n",
      "----------Random Action----------\n",
      "fps: 8.408401376841084\n",
      "TIMESTEP 297 / STATE explore / EPSILON 0.0998041959999997 / ACTION 0 / REWARD 0.1 / Q_MAX  8.235215 / Loss  0.33286795020103455\n",
      "fps: 6.6696625497127355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 298 / STATE explore / EPSILON 0.0998031969999997 / ACTION 1 / REWARD 0.1 / Q_MAX  7.5408955 / Loss  0.1015625\n",
      "fps: 6.34794637478925\n",
      "TIMESTEP 299 / STATE explore / EPSILON 0.0998021979999997 / ACTION 1 / REWARD 0.1 / Q_MAX  6.4140463 / Loss  0.07485312223434448\n",
      "fps: 7.215721356402241\n",
      "TIMESTEP 300 / STATE explore / EPSILON 0.0998011989999997 / ACTION 1 / REWARD 0.1 / Q_MAX  6.896754 / Loss  0.09592065215110779\n",
      "fps: 6.489154583542197\n",
      "TIMESTEP 301 / STATE explore / EPSILON 0.0998001999999997 / ACTION 1 / REWARD 0.1 / Q_MAX  7.3138676 / Loss  0.12950068712234497\n",
      "fps: 6.611033352247651\n",
      "TIMESTEP 302 / STATE explore / EPSILON 0.0997992009999997 / ACTION 1 / REWARD 0.1 / Q_MAX  3.6535587 / Loss  1.181394338607788\n",
      "fps: 7.482372921267737\n",
      "TIMESTEP 303 / STATE explore / EPSILON 0.0997982019999997 / ACTION 1 / REWARD -1 / Q_MAX  8.722599 / Loss  0.7250130772590637\n",
      "----------Random Action----------\n",
      "fps: 6.732214054123463\n",
      "TIMESTEP 304 / STATE explore / EPSILON 0.0997972029999997 / ACTION 1 / REWARD 0.1 / Q_MAX  6.592033 / Loss  1.2247620820999146\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4829a0a02061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-12216cee503d>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m(observe)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fd5fdecc2027>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[0;34m(model, game_state, observe)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#run the selected action and observed next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fps: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# helpful for measuring frame rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-35ecfd7f7a27>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initiliaze the display coroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mactions_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# storing actions in a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    449\u001b[0m                                        name=indexer)\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6183\u001b[0m             other = DataFrame(other.values.reshape((1, len(other))),\n\u001b[1;32m   6184\u001b[0m                               \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6185\u001b[0;31m                               columns=combined_columns)\n\u001b[0m\u001b[1;32m   6186\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
